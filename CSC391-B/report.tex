\documentclass{article}
\title{CSC391 project2 report}
\author{Alex Zhang}
\date{Oct 2023}
\textwidth=16.00cm 
\textheight=22.00cm 
\topmargin=0.00cm
\oddsidemargin=0.00cm 
\evensidemargin=0.00cm 
\headheight=0cm 
\headsep=0.5cm
\textheight=610pt
\usepackage{graphicx}
\usepackage{multicol}

\graphicspath{ {./images/} }

\usepackage{latexsym,array,delarray,amsthm,amssymb,epsfig}
\usepackage{amsmath}
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily,
  mathescape
}

\newcommand{\bmat}[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand{\mat}[1]{\mathbf{#1}}

\let\ds\displaystyle

\begin{document}
\maketitle
\section*{Part1}
\subsection*{Rotation}
In every case I tried to rotate image to 270 degree clockwise.\\
For SIFT, I think the result shows that SIFT is rotation invariant after drawing the matches between two images.
The keypoints in bed sheet and human face can match with each other and the result shows that the accuracy is around 65\%.
The result matches textbook's result.\\
\\
For FAST, in this case, the number of keypoints is already different.
After trying to create matching, I will say that FAST is not rotation invariant.
I have to increase the threshold to make my code consider good matches.\\
\\
For Harris Corner Detection, there is only few matches.
I read from some online resources said that Harris should be rotation invariant, so I check whether it will match without any rotation.
If there is no rotation, all keypoints are matched, but if there is a 270 rotation, really a few matched.
One probability is that there seems to be some identical corners like wood corners on both windows, It may confuse the detection.
Second probability is that my hair seems to be smooth, it is hard to draw corner from harris detection and the descriptor may also confused.\\
\\
Using ORB seems to create many matches even if I rotate 270 degree.
It has better accuracy than using SIFT.
One explanation for this is that when rotation degree is proportional to 90, ORB outperforms SIFT because the features for ORB usually stick to the center of image.
This will make feature matching easier because the position of features will always be vertical or horizontal.





\subsection*{Scale}
To test the scale invariant, I first shrink the image to 0.6 of the original.\\
For SIFT, I think it can still draw many matches with given keypoints, but compare to the original image, it still has many dismatches.
I consider SIFT as scale invariant because the accuracy is about 33\%.
The result in textbook is about 40\%
One reason I think when SIFT didn't perform well when shrinking the image than expanding image is that, the number of keypoins decreases a lot when shrinking.
The percentage will decrease.\\
\\
I think FAST is scale invariant.
In my image, to say whether FAST is robust on scaling depends on threshold.
In my case, there are lots of keypoints missing after I shrinked the image.
This will decrease the accuracy for finding matching features.
\\
For Harris corner detection, I think it also didn't do well if I try to shrink the image.
We can see that there are many dismatches if I only took part of matches.
Also the for the features that seems to be matched, my setting of threshold didn't count most of it because they are not close enough
to be accepted by threshold.
I can say Harris is not scale invariant.\\
\\
For ORB, I think it is scale invariant and it has even better result than SIFT.
One reason for this is that ORB has fewer features than SIFT and most of the features lie on that stripe pillow.
After shrinked the image, features on stripe pillow still preserved which I think this is the main reason of having better result than SIFT.



\subsection*{Low-light illumination}
I tried to make testing image to decrease the illumination.\\
For SIFT, the result indicates that SIFT is not really robust on low-light illumination.
Since there is no scaling or rotation, if we see a parallel line and the destination is the same, it is a good match.
Based on the matching feature, there are not so many parallel lines.\\
\\
For FAST, I saw many parallel lines.
I still think many features matches with each other though there are some mismatches.
FAST should be robust to low-light illumination.
\\
For Harris, I think it is doing fine for low-light illumination.
There are many matches after I decrease the illumination.
I think the change for gradient on corner after decreasing light can still be detected for Harris.
So Harris corner detection should be robust for low-light illumination.\\
\\
For ORB, It is hard to say ORB is robust in low-light illumination or not.
It is true that there are some parallel lines, but there are also many intersections.
Based on my observation, it has similar performance with SIFT which is also what textbook shows.
I think ORB can be robust for low-light illumination but not good as FAST and Harris in this image.
\section*{Part2}
\subsection*{SIFT}
the following image is using SIFT as detector and descriptor.
\begin{figure}[!ht]
  \begin{center}
    \includegraphics[scale=0.4]{SIFT-result.jpg}
    \label{fig:boat1}
  \end{center} 
\end{figure}

\subsection*{BRISK}

\end{document}