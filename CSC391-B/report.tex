\documentclass{article}
\title{CSC391 project2 report}
\author{Alex Zhang}
\date{Oct 2023}
\textwidth=16.00cm 
\textheight=22.00cm 
\topmargin=0.00cm
\oddsidemargin=0.00cm 
\evensidemargin=0.00cm 
\headheight=0cm 
\headsep=0.5cm
\textheight=610pt
\usepackage{graphicx}
\usepackage{multicol}

\graphicspath{ {./images/} }

\usepackage{latexsym,array,delarray,amsthm,amssymb,epsfig}
\usepackage{amsmath}
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily,
  mathescape
}

\newcommand{\bmat}[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand{\mat}[1]{\mathbf{#1}}

\let\ds\displaystyle

\begin{document}
\maketitle
\section*{Part1}
\subsection*{Rotation}
For SIFT, I tried to rotate the image counterclockwise by 45 degree.
I think the result shows that SIFT is rotation invariant. 
When running python rotation, it is true that after rotated the image, the number of features increases.
However, I think it didn't really change the overall detection. 
It still can draw features without any difficulties.\\
\\
For FAST, I tried to rotate image 90 degree clockwisely.
I think in this case, FAST has even better rotation invariant than SIFT.
MOst of the features didn't change even if I increase the rotation degree.
Currently, one interesting thing I found is that there is a stripe pillow on my left side.
Both SIFt and FAST seem to have to many features on that pillow. 
It is probably the size of that pillow is small so that these algorithm cannot draw keypoints too specificly.\\
\\
For Harris Corner Detection, I also rotate the image to 90 degree to compare.
It turns out Harris Corner Detection is also rotation invariant.
It can succesfully detect corner even if the image changes direction.
One interesting stuff for Harris's idea is that it didn't count the top of my hair under the light as corner whearas it detects my hair close to my shoulder to be corner.
I think this is because the illumination makes my hair so bright which Harris detection didn't count it.\\
\\
For ORB, I also rotate the image to 90 degree.
I can also tell that ORB is rotation invariant because keypoints didn't change after doing the rotation.
For the same stripe pillow problem, I think this time ORB performs better than the begining two.
\subsection*{Scale}
To test the scale invariant, I first shrink the image and then perform Gaussian Blur with a 5 by 5 kernel.\\
For SIFT, I think it can still draw some part of the keypoints, but compare to the original image, it still loses many keypoints.
I don't think SIFT algorithm did well when adding blurring and shrinking the size compare to other methods like ORB.
It is different from what I know about this algorithm, because it should be scale-invariant.
After reading the textbook, I found that SIFT algorithm perform worse when shrinking than magnifying the image and I think blurring also make SIFT unable to present ideal result.\\
\\
For FAST, from my result, I think FAST algorithm is not invariant to blurring.
After I shrinked and blurred my image, I found that FAST didn't detect some part of my hair and some parts of the roof.
Based on this, I look at the description of how FAST is actually doing.
It is just taking pixels around the testing point and check whether it exceeds the threashold. 
Blurring will definitely affect the detection because each pixel value is changed based on the around values.\\
\\
For Harris corner detection, I think it also didn't do well if I try to blur the image.
Part of my hair was not detected for blurred image.
I think this is because after blurring, the intensity for each pixel is mixed.
In this case, the edge and corner can also be mixed and Harris detection failed.\\
\\
For ORB, I think it has the best performance after shrinking and blurring the image.
I took a look at the repeatability diagram in textbook and it shows that ORB is invariant in blurring and doing fine if the image got shrink.



\subsection*{Low-light illumination}
I tried to make testing image to decrease the illumination.\\
For SIFT, I think it can still have really good detection even if I decrease the illumination.
I will say that SIFT is robust for Low-light illumination.\\
\\
For FAST, it has many more keypoints in low-illumination image than the original image. 
It even lost many keypoints on my face. 
I think the problem here is that for any pixel, if its surrounding intensity is low, FAST will not detect correctly.
It will add keypoints if a pixel used to has high illumination because the contrast increase.
FAST is not really robust for low-light illumination from my experiment.\\
\\
For Harris, I think it is doing fine for low-light illumination.
Harris can still detect many corners when I decrease the illumination.
I think the change for gradient on corner after decreasing light can still be detected for Harris.
So Harris corner detection should be robust for low-light illumination.\\
\\
For ORB, I think it can still work well with low-light image.
It lost some keypoints on my face but for the rest part it can still detect.
I think ORB is robust for low-light illumination.
\section*{Part2}


\end{document}