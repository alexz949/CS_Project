\documentclass{article}
\title{CSC391 project2 report}
\author{Alex Zhang}
\date{Oct 2023}
\textwidth=16.00cm 
\textheight=22.00cm 
\topmargin=0.00cm
\oddsidemargin=0.00cm 
\evensidemargin=0.00cm 
\headheight=0cm 
\headsep=0.5cm
\textheight=610pt
\usepackage{graphicx}
\usepackage{multicol}

\graphicspath{ {./images/} }

\usepackage{latexsym,array,delarray,amsthm,amssymb,epsfig}
\usepackage{amsmath}
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily,
  mathescape
}

\newcommand{\bmat}[1]{\begin{bmatrix} #1 \end{bmatrix}}
\newcommand{\mat}[1]{\mathbf{#1}}

\let\ds\displaystyle

\begin{document}
\maketitle
\section*{Part1}
\subsection*{Rotation}
For SIFT, I tried to rotate the image clockwisely 135 degree.
I think the result shows that SIFT is rotation invariant after drawing the matches between two images.
The keypoints in bed sheet and human face can match with each other.
There are also some dismatches like firealert matches to a fan.
I think just based on this figure, it matches what textbook diagram's result.
I think the percentage of matches depends on different image and that percentage will change a lot.\\
\\
For FAST, I also tried to rotate 135 degree
In this case, the number of keypoints is already different.
After trying to creat matching, I found out that keypoints are not really match to the correct keypoints.
I think FAST algorithm performs worse than SIFT when rotating the image or even nort rotation invariant.\\
\\
For Harris Corner Detection, I did the same rotation.
It turns out Harris Corner Detection seems to be rotation invariant.
It can succesfully detect corners even if the image changes direction.
The number of detectors seem to be the same before or after rotation which is kind of interesting.
Also, I found out that for both images Harris didn't detect my hair close to light.
I think this is because the illumination makes my hair so bright which Harris detection didn't count it.\\
\\
For ORB, same thing happened.
For ORB, I could say that ORB is rotation invariant, but when rotatring 135 degree, SIFT has better accuracy than ORB.
There are some matches for pillow and human face.
However, there are also some dismatches for the fan and bed sheet.
This result also matches the result in textbook which the rotation invariant for ORB is slightly worser than SIFT.




\subsection*{Scale}
To test the scale invariant, I first shrink the image to 0.6 of the original.\\
For SIFT, I think it can still draw many matches with given keypoints, but compare to the original image, it still has many dismatches.
I consider SIFT as scale invariant because the number of matches fits the result in textbook diagram.
One reason I think when SIFT didn't perform well when shrinking the image than expanding image is that, the number of keypoins decreases a lot when shrinking.
Even every existing keypoint in shrinked image matches, the percentage will still not be so large.\\
\\
For FAST, from my result, I think FAST is somehow scale invariant.
I think it is true that some keypoints are missing if I shrinked the image but overall, I think it can still matches most of the keypoints.
I even think for this case, FAST is doing better than SIFT if just just shrink the image.
Based on this, I look at the description of how FAST is actually doing.
It is just taking pixels around the testing point and check whether it exceeds the threashold. 
IF we did not shrink an image a lot, the intensity for each pixel may not change a lot.\\
\\
For Harris corner detection, I think it also didn't do well if I try to shrink the image.
Part of my hair was not detected and there are some other keypoints added in shrinked image.
I think this is because after shrinking, the intensity for each pixel is mixed.
In this case, the edge and corner can also be mixed and Harris detection failed.\\
\\
For ORB, I think it is somehow scale invariant because it can still succesfully detect some objests even if I shrinked the image.
However, I think it did not perform well compared to SIFT because it cannot detect human face and has lots of wrong matches in pillow and bed sheet.




\subsection*{Low-light illumination}
I tried to make testing image to decrease the illumination.\\
For SIFT, it doesn't perform well if I decrease the illumination of the image.
Usually, since I didn't rotate or shrink the image, the matches can be easily seen by counting parallel lines.
In the test image, there are many parallel lines but there are also many interections.
As textbook said, SIFT's matches can change a lot if it is a low-light illumination.\\
\\
For FAST, I saw many parallel lines, which indicates that many keypoints should be a match.
After closer examination, indeed there are some keypoint that is not match but still are parallel to each other.
I still think many keypoints matches in two images.
It is true FAST lost some keypoints on human face but I think this is acceptable.
FAST should be robust to low-light illumination.
\\
For Harris, I think it is doing fine for low-light illumination.
Harris can still detect many corners when I decrease the illumination.
I think the change for gradient on corner after decreasing light can still be detected for Harris.
So Harris corner detection should be robust for low-light illumination.\\
\\
For ORB, I think it can still work fine with low-light image.
It is true that there are some interections between lines, most rest lines work well.
Based on my observation, it has better performance than SIFT which is also what textbook shows.
I think ORB is robust for low-light illumination.
\section*{Part2}


\end{document}